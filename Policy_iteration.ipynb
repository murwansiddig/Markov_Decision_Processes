{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Policy Iteration Algorithm\n",
    "[Murwan Siddig](mailto:msiddig@clemson.edu)\n",
    "\n",
    "---------------------------------------------------------------------------------------------------\n",
    "\n",
    "### Exercise 6.48.\n",
    "[Puterman, Martin L. Markov decision processes: discrete stochastic dynamic programming.](https://books.google.com/books?hl=en&lr=&id=VvBjBAAAQBAJ&oi=fnd&pg=PT9&dq=puterman+markov+decision+processes&ots=rsgDAORYMN&sig=27nnDn3J3p1ytP_9Khd31OELPkc#v=onepage&q=puterman%20markov%20decision%20processes&f=false)\n",
    "\n",
    "**(Rosenthal, White, and Young, 1978)**. Consider the following version of the dynamic location model presented in Problem 3.17. There are $Q = 4$ work sites, with site 1 denoting the home office and 2, 3, and 4 denoting remote sites. The cost of relocating the equipment trailer is $d(k,j) = 300$ for $k \\neq j$; the cost $c(k,j)$ of using the equipment trailer is 100 if the work force is at site $k > 1$, and trailer is at site $j \\neq k$ with $j>1; 50$ if $j=k$ and $j>1$ and 200 if the work force is at remote site $j > 1$, and the trailer is at the home office, site 1. If the work force is at site 1, no work is carried out, so the cost of using the trailer in this case can be regarded to be 0. Assume that the probability of moving between sites in one period $p(j l s)$ is given by the matrix $P$\n",
    "\n",
    "\\begin{bmatrix}\n",
    "0 & 0.3 & 0.3 & 0.3 \\\\\n",
    "0 & 0.5 & 0.5 & 0 \\\\\n",
    "0 & 0 & 0.8 & 0.2 \\\\\n",
    "0.4 & 0 & 0 & 0.6 \\\\\n",
    "\\end{bmatrix}\n",
    "\n",
    "Assuming the discount rate $\\lambda = 0.95$, find a relocation policy which minimizes the expected discounted cost and describe the structure of the optimal policy.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#technical lines\n",
    "using LinearAlgebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#workforce transition probability from stat s to j\n",
    "P = [0.1 0.3 0.3 0.3;\n",
    "     0.0 0.5 0.5 0.0;\n",
    "     0.0 0.0 0.8 0.2;\n",
    "     0.4 0.0 0.0 0.6];\n",
    "\n",
    "#cost of obtaining material when workforce is in facility \n",
    "#j and trailer is in facility j and trailer is in site m\n",
    "Cost = [0 200 200 200;\n",
    "        0 50 100 100;\n",
    "        0 100 50 100;\n",
    "        0 100 100 50];\n",
    "\n",
    "#cost of reallocating the trailer \n",
    "f = 300; \n",
    "\n",
    "#number of locations  \n",
    "Q = 4;\n",
    "\n",
    "#discount rate \n",
    "lambda =0.95;\n",
    "\n",
    " \n",
    "#states are define as (location of workforce, location of trailer) \n",
    "#(1,1), (1,2), ... (4,4)\n",
    "#number of states\n",
    "nbstates =16;\n",
    "\n",
    "Id = Matrix(1I, nbstates, nbstates);\n",
    "\n",
    "index = [];\n",
    "for i=1:Q\n",
    "    for j=1:Q\n",
    "        temp = [];\n",
    "        push!(temp,i)\n",
    "        push!(temp,j)\n",
    "        push!(index,temp)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Policy_Iteration (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function Policy_Iteration()\n",
    "    #step1 initialization \n",
    "    PP = zeros(nbstates,nbstates);\n",
    "    val_cost = zeros(nbstates);\n",
    "    #we start with an arbitrary decesion rule ....\n",
    "    #take action 2 no matter what stage (move trailer to location 2)\n",
    "    dec = fill(2, nbstates);\n",
    "\n",
    "    iter = 0;\n",
    "    V = zeros(nbstates)\n",
    "\n",
    "    #Transition Probability\n",
    "    for s=1:nbstates\n",
    "        count = 0;\n",
    "        #Immediate cost \n",
    "        immed_cost = 0;\n",
    "        if index[s][2] != dec[s]\n",
    "            immed_cost +=f\n",
    "        end\n",
    "        for j=1:Q\n",
    "            immed_cost +=Cost[dec[s],j]*P[index[s][1],j]\n",
    "        end\n",
    "        val_cost[s] = immed_cost\n",
    "        for i=1:Q\n",
    "            for j=1:Q\n",
    "                count +=1\n",
    "                if index[count][2] != dec[s]\n",
    "                    PP[s,count] = 0;\n",
    "                else\n",
    "                    PP[s,count] = P[index[s][1],index[count][1]];\n",
    "                end        \n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    while true\n",
    "        iter += 1\n",
    "\n",
    "        dec_copy = zeros(nbstates);\n",
    "        for s=1:nbstates\n",
    "            dec_copy[s]=dec[s]\n",
    "        end\n",
    "\n",
    "        #step1 policy evaluation  \n",
    "        V = inv(Id-lambda*PP)*val_cost;\n",
    "\n",
    "        #Step2 policy improvment\n",
    "        counter = 0;\n",
    "        for s=1:nbstates\n",
    "            action = zeros(Q);\n",
    "            for a=1:Q\n",
    "                count = 0;\n",
    "                for i=1:Q\n",
    "                    for j=1:Q\n",
    "                        count +=1\n",
    "                        if index[count][2] != a\n",
    "                            PP[s,count] = 0;\n",
    "                        else\n",
    "                            PP[s,count] = P[index[s][1],index[count][1]];\n",
    "                        end        \n",
    "                    end\n",
    "                end\n",
    "\n",
    "                if a==index[s][2]\n",
    "                    action[a] = Cost[a,:]'P[index[s][1],:]+lambda*PP[s,:]'*V\n",
    "                else\n",
    "                    action[a] =f+Cost[a,:]'P[index[s][1],:]+lambda*PP[s,:]'*V\n",
    "                end\n",
    "            end\n",
    "\n",
    "            #update the decesion rule\n",
    "            if findmin(action)[2] == dec[s]\n",
    "                counter +=1\n",
    "            end\n",
    "            dec[s] = findmin(action)[2]\n",
    "            V[s] = findmin(action)[1]\n",
    "        end\n",
    "        println(\"iter=\",iter)\n",
    "        println(\"val_cost=\",val_cost)\n",
    "        println(\"Previous decesion Rule=\",dec_copy)\n",
    "        println(\"New decesion Rule=\",dec)\n",
    "        println(\"Optimal Value=\", V)\n",
    "        println(\"========================================\")\n",
    "\n",
    "        if dec == dec_copy\n",
    "        #if counter == nbstates\n",
    "            break;\n",
    "        end\n",
    "\n",
    "        #Transition Probability\n",
    "        for s=1:nbstates\n",
    "            count = 0;\n",
    "            #Immediate cost \n",
    "            immed_cost = 0;\n",
    "            if index[s][2] != dec[s]\n",
    "                immed_cost +=f\n",
    "            end\n",
    "            for j=1:Q\n",
    "                immed_cost +=Cost[dec[s],j]*P[index[s][1],j]\n",
    "            end\n",
    "            val_cost[s] = immed_cost\n",
    "            for i=1:Q\n",
    "                for j=1:Q\n",
    "                    count +=1\n",
    "                    if index[count][2] != dec[s]\n",
    "                        PP[s,count] = 0;\n",
    "                    else\n",
    "                        PP[s,count] = P[index[s][1],index[count][1]];\n",
    "                    end        \n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    println(\"Number of iterations=\", iter)\n",
    "    println(\"Optimal policy=\", dec)\n",
    "    println(\"Optimal Value=\", V)\n",
    "    println(\"=====================================\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=1\n",
      "val_cost=[375.0, 75.0, 375.0, 375.0, 375.0, 75.0, 375.0, 375.0, 400.0, 100.0, 400.0, 400.0, 360.0, 60.0, 360.0, 360.0]\n",
      "Previous decesion Rule=[2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0]\n",
      "New decesion Rule=[2, 2, 3, 4, 2, 2, 3, 2, 2, 2, 3, 4, 2, 2, 3, 4]\n",
      "Optimal Value=[1915.65, 1615.65, 1900.65, 1900.65, 1942.47, 1642.47, 1927.47, 1942.47, 1957.46, 1657.46, 1902.46, 1932.46, 1867.32, 1567.32, 1846.62, 1816.62]\n",
      "========================================\n",
      "iter=2\n",
      "val_cost=[375.0, 75.0, 75.0, 75.0, 375.0, 75.0, 75.0, 375.0, 400.0, 100.0, 60.0, 90.0, 360.0, 60.0, 60.0, 30.0]\n",
      "Previous decesion Rule=[2.0, 2.0, 3.0, 4.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 3.0, 4.0, 2.0, 2.0, 3.0, 4.0]\n",
      "New decesion Rule=[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 4]\n",
      "Optimal Value=[1584.66, 1584.66, 1284.66, 1584.66, 1582.16, 1582.16, 1282.16, 1582.16, 1559.23, 1559.23, 1259.23, 1559.23, 1574.82, 1555.55, 1274.82, 1527.96]\n",
      "========================================\n",
      "iter=3\n",
      "val_cost=[375.0, 375.0, 75.0, 375.0, 375.0, 375.0, 75.0, 375.0, 360.0, 360.0, 60.0, 360.0, 360.0, 60.0, 60.0, 30.0]\n",
      "Previous decesion Rule=[3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 2.0, 3.0, 4.0]\n",
      "New decesion Rule=[3, 2, 3, 4, 3, 2, 3, 3, 3, 3, 3, 4, 3, 2, 3, 4]\n",
      "Optimal Value=[1584.66, 1559.72, 1284.66, 1539.84, 1582.16, 1567.16, 1282.16, 1582.16, 1559.23, 1559.23, 1259.23, 1554.35, 1574.82, 1530.46, 1274.82, 1453.13]\n",
      "========================================\n",
      "iter=4\n",
      "val_cost=[375.0, 75.0, 75.0, 75.0, 375.0, 75.0, 75.0, 375.0, 360.0, 360.0, 60.0, 90.0, 360.0, 60.0, 60.0, 30.0]\n",
      "Previous decesion Rule=[3.0, 2.0, 3.0, 4.0, 3.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 4.0, 3.0, 2.0, 3.0, 4.0]\n",
      "New decesion Rule=[3, 2, 3, 4, 3, 2, 3, 4, 3, 3, 3, 4, 3, 2, 3, 4]\n",
      "Optimal Value=[1584.66, 1534.01, 1284.66, 1473.09, 1582.16, 1553.59, 1282.16, 1545.42, 1559.23, 1559.23, 1259.23, 1460.83, 1574.82, 1495.17, 1274.82, 1371.57]\n",
      "========================================\n",
      "iter=5\n",
      "val_cost=[375.0, 75.0, 75.0, 75.0, 375.0, 75.0, 75.0, 100.0, 360.0, 360.0, 60.0, 90.0, 360.0, 60.0, 60.0, 30.0]\n",
      "Previous decesion Rule=[3.0, 2.0, 3.0, 4.0, 3.0, 2.0, 3.0, 4.0, 3.0, 3.0, 3.0, 4.0, 3.0, 2.0, 3.0, 4.0]\n",
      "New decesion Rule=[3, 2, 3, 4, 3, 2, 3, 4, 3, 3, 3, 4, 3, 2, 3, 4]\n",
      "Optimal Value=[1584.66, 1534.01, 1284.66, 1400.13, 1582.16, 1553.59, 1282.16, 1465.99, 1559.23, 1559.23, 1259.23, 1409.78, 1574.82, 1495.17, 1274.82, 1307.09]\n",
      "========================================\n",
      "Number of iterations=5\n",
      "Optimal policy=[3, 2, 3, 4, 3, 2, 3, 4, 3, 3, 3, 4, 3, 2, 3, 4]\n",
      "Optimal Value=[1584.66, 1534.01, 1284.66, 1400.13, 1582.16, 1553.59, 1282.16, 1465.99, 1559.23, 1559.23, 1259.23, 1409.78, 1574.82, 1495.17, 1274.82, 1307.09]\n",
      "=====================================\n",
      "  3.510033 seconds (4.56 M allocations: 229.429 MiB, 3.96% gc time)\n"
     ]
    }
   ],
   "source": [
    "@time Policy_Iteration();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------**\n",
    "**-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------**\n",
    "\n",
    "\n",
    "### Exercise 6.63.\n",
    "[Puterman, Martin L. Markov decision processes: discrete stochastic dynamic programming.](https://books.google.com/books?hl=en&lr=&id=VvBjBAAAQBAJ&oi=fnd&pg=PT9&dq=puterman+markov+decision+processes&ots=rsgDAORYMN&sig=27nnDn3J3p1ytP_9Khd31OELPkc#v=onepage&q=puterman%20markov%20decision%20processes&f=false)\n",
    "\n",
    "**(A simple bandit model).** A decision maker observes a discrcte-time system which moves between states $(s_1, s_2, s_3, s_4)$ according to the following transition probability matrix $P$\n",
    "\n",
    "\\begin{bmatrix}\n",
    "0.3 & 0.4 & 0.2 & 0.1 \\\\\n",
    "0.2 & 0.3 & 0.5 & 0.0  \\\\\n",
    "0.1 & 0.0 & 0.8 & 0.1 \\\\\n",
    "0.4 & 0.0 & 0.0 & 0.6 \\\\\n",
    "\\end{bmatrix}\n",
    "\n",
    "At each point of time, the decision maker may leave the system and receive a reward of $R = 20$ units, or alternatively remain in the system and receive a reward of $r(s_i)$ units if the system occupies state $s_i$. If the decision maker decides to remain in the system, its state at the next decision epoch is determined by $P$.\n",
    "Assume a discount rate of $\\lambda = 0.9$ and that $r(s_i)=i$.\n",
    "\n",
    "-----------------------------------------------------------------------------------------\n",
    "\n",
    "### Use the policy iteration to find a stationary policy which maximizes the expected total discounted reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = [0.3 0.4 0.2 0.1;\n",
    "     0.2 0.3 0.5 0.0 ;\n",
    "     0.1 0.0 0.8 0.1;\n",
    "     0.4 0.0 0.0 0.6];\n",
    "\n",
    "reward = [1 2 3 4 0];\n",
    "\n",
    "nbstates  = 5;\n",
    "R = 20;\n",
    "\n",
    "nbaction =2; \n",
    "\n",
    "lambda =0.9;\n",
    "Id = Matrix(1I, nbstates, nbstates);\n",
    "dec = fill(0, nbstates);\n",
    "dec[5] = 0;\n",
    "PP = zeros(nbstates,nbstates);\n",
    "PP[5,5]=1;\n",
    "\n",
    "imme_reward = zeros(nbstates);\n",
    "\n",
    "iter = 0;\n",
    "V = zeros(nbstates)\n",
    "\n",
    "#transition probability \n",
    "\n",
    "for s=1:nbstates-1\n",
    "    if dec[s]==1\n",
    "        imme_reward[s] = reward[s]\n",
    "        for ss=1:nbstates-1\n",
    "            PP[s,ss]=P[s,ss]\n",
    "            PP[s,5] = 0\n",
    "        end\n",
    "    else\n",
    "        imme_reward[s] = R\n",
    "        for ss=1:nbstates-1\n",
    "            PP[s,ss]=0\n",
    "            PP[s,5]=1\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=1\n",
      "Previous decesion Rule=[0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "New decesion Rule=[2, 1, 1, 1, 0]\n",
      "imme_reward=[20.0, 2.0, 3.0, 4.0, 0.0]\n",
      "Optimal Value=[20.0, 20.0, 21.0, 22.0, 0.0]\n",
      "========================================\n",
      "iter=2\n",
      "Previous decesion Rule=[2.0, 1.0, 1.0, 1.0, 0.0]\n",
      "New decesion Rule=[1, 1, 1, 1, 0]\n",
      "imme_reward=[1.0, 2.0, 3.0, 4.0, 0.0]\n",
      "Optimal Value=[21.3884, 23.313, 25.0939, 24.8477, 0.0]\n",
      "========================================\n",
      "iter=3\n",
      "Previous decesion Rule=[1.0, 1.0, 1.0, 1.0, 0.0]\n",
      "New decesion Rule=[1, 1, 1, 1, 0]\n",
      "imme_reward=[1.0, 2.0, 3.0, 4.0, 0.0]\n",
      "Optimal Value=[24.0775, 25.5087, 27.3053, 27.5389, 0.0]\n",
      "========================================\n",
      "Number of iterations=3\n",
      "Optimal policy=[1, 1, 1, 1, 0]\n",
      "Optimal Value=[24.0775, 25.5087, 27.3053, 27.5389, 0.0]\n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "while true \n",
    "    iter += 1\n",
    "    dec_copy = zeros(nbstates);\n",
    "    for s=1:nbstates\n",
    "        dec_copy[s]=dec[s]\n",
    "    end\n",
    "    \n",
    "    #step1 policy evaluation  \n",
    "    V = inv(Id-lambda*PP)*imme_reward;\n",
    "    for s=1:nbstates-1\n",
    "        action = zeros(nbaction);\n",
    "        for a=1:nbaction\n",
    "            if a==1\n",
    "                imme_reward[s] = reward[s]\n",
    "                for ss=1:nbstates-1\n",
    "                    PP[s,ss]=P[s,ss]\n",
    "                    PP[s,5] = 0\n",
    "                end\n",
    "            else\n",
    "                imme_reward[s] = R\n",
    "                for ss=1:nbstates-1\n",
    "                    PP[s,ss]=0\n",
    "                    PP[s,5]=1\n",
    "                end\n",
    "            end\n",
    "            action[a] = imme_reward[s]+lambda*PP[s,:]'*V\n",
    "        end\n",
    "        dec[s] = findmax(action)[2]\n",
    "        V[s] = findmax(action)[1]\n",
    "        if dec[s]==1\n",
    "            imme_reward[s] = reward[s]\n",
    "        else\n",
    "            imme_reward[s] = R\n",
    "        end \n",
    "    end\n",
    "    println(\"iter=\",iter)\n",
    "    println(\"Previous decesion Rule=\",dec_copy)\n",
    "    println(\"New decesion Rule=\",dec)\n",
    "    println(\"imme_reward=\",imme_reward)\n",
    "    println(\"Optimal Value=\", V)\n",
    "    println(\"========================================\")\n",
    "    \n",
    "    if dec == dec_copy\n",
    "    #if counter == nbstates\n",
    "        break;\n",
    "    end\n",
    "    \n",
    "    \n",
    "    #transition probability \n",
    "\n",
    "    for s=1:nbstates-1\n",
    "        if dec[s]==1\n",
    "            imme_reward[s] = reward[s]\n",
    "            for ss=1:nbstates-1\n",
    "                PP[s,ss]=P[s,ss]\n",
    "                PP[s,5] = 0\n",
    "            end\n",
    "        else\n",
    "            imme_reward[s] = R\n",
    "            for ss=1:nbstates-1\n",
    "                PP[s,ss]=0\n",
    "                PP[s,5]=1\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end \n",
    "\n",
    "println(\"Number of iterations=\", iter)\n",
    "println(\"Optimal policy=\", dec)\n",
    "println(\"Optimal Value=\", V)\n",
    "println(\"=====================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the smallest value of $R$ so that it is optimal to leave the system in state 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec = fill(0, nbstates);\n",
    "dec[5] = 2;\n",
    "PP = zeros(nbstates,nbstates);\n",
    "PP[5,5]=1;\n",
    "\n",
    "imme_reward = zeros(nbstates);\n",
    "\n",
    "iter = 0;\n",
    "V = zeros(nbstates)\n",
    "\n",
    "#transition probability \n",
    "\n",
    "for s=1:nbstates-1\n",
    "    if dec[s]==1\n",
    "        imme_reward[s] = reward[s]\n",
    "        for ss=1:nbstates-1\n",
    "            PP[s,ss]=P[s,ss]\n",
    "            PP[s,5] = 0\n",
    "        end\n",
    "    else\n",
    "        imme_reward[s] = R\n",
    "        for ss=1:nbstates-1\n",
    "            PP[s,ss]=0\n",
    "            PP[s,5]=1\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=1\n",
      "Previous decesion Rule=[0.0, 0.0, 0.0, 0.0, 2.0]\n",
      "New decesion Rule=[0, 1, 1, 1, 0]\n",
      "imme_reward=[20.0, 2.0, 3.0, 4.0, 0.0]\n",
      "Optimal Value=[20.0, 20.0, 21.0, 22.0, 0.0]\n",
      "========================================\n",
      "iter=2\n",
      "Previous decesion Rule=[0.0, 1.0, 1.0, 1.0, 0.0]\n",
      "New decesion Rule=[1, 1, 1, 1, 0]\n",
      "imme_reward=[1.0, 2.0, 3.0, 4.0, 0.0]\n",
      "Optimal Value=[21.3884, 23.313, 25.0939, 24.8477, 0.0]\n",
      "========================================\n",
      "iter=3\n",
      "Previous decesion Rule=[1.0, 1.0, 1.0, 1.0, 0.0]\n",
      "New decesion Rule=[1, 1, 1, 1, 0]\n",
      "imme_reward=[1.0, 2.0, 3.0, 4.0, 0.0]\n",
      "Optimal Value=[24.0775, 25.5087, 27.3053, 27.5389, 0.0]\n",
      "========================================\n",
      "iter=4\n",
      "Previous decesion Rule=[1.0, 2.0, 1.0, 1.0, 0.0]\n",
      "New decesion Rule=[0, 1, 1, 1, 0]\n",
      "imme_reward=[25.5087, 2.0, 3.0, 4.0, 0.0]\n",
      "Optimal Value=[25.5087, 25.7663, 27.4341, 28.0541, 0.0]\n",
      "========================================\n",
      "iter=5\n",
      "Previous decesion Rule=[0.0, 1.0, 1.0, 1.0, 0.0]\n",
      "New decesion Rule=[0, 1, 1, 1, 0]\n",
      "imme_reward=[25.5087, 2.0, 3.0, 4.0, 0.0]\n",
      "Optimal Value=[25.5087, 26.3671, 28.1253, 28.659, 0.0]\n",
      "========================================\n",
      "iter=6\n",
      "Previous decesion Rule=[0.0, 2.0, 1.0, 1.0, 0.0]\n",
      "New decesion Rule=[0, 1, 1, 1, 0]\n",
      "imme_reward=[26.3671, 2.0, 3.0, 4.0, 0.0]\n",
      "Optimal Value=[26.3671, 26.7429, 28.6172, 29.3307, 0.0]\n",
      "========================================\n",
      "iter=7\n",
      "Previous decesion Rule=[0.0, 1.0, 1.0, 1.0, 0.0]\n",
      "New decesion Rule=[0, 1, 1, 1, 0]\n",
      "imme_reward=[26.3671, 2.0, 3.0, 4.0, 0.0]\n",
      "Optimal Value=[26.3671, 26.8819, 28.6172, 29.3307, 0.0]\n",
      "========================================\n",
      "iter=8\n",
      "Previous decesion Rule=[0.0, 2.0, 1.0, 1.0, 0.0]\n",
      "New decesion Rule=[0, 1, 1, 1, 0]\n",
      "imme_reward=[26.8819, 2.0, 3.0, 4.0, 0.0]\n",
      "Optimal Value=[26.8819, 27.1073, 28.9122, 29.7337, 0.0]\n",
      "========================================\n",
      "iter=9\n",
      "Previous decesion Rule=[0.0, 1.0, 1.0, 1.0, 0.0]\n",
      "New decesion Rule=[0, 1, 1, 1, 0]\n",
      "imme_reward=[26.8819, 2.0, 3.0, 4.0, 0.0]\n",
      "Optimal Value=[26.8819, 27.1907, 28.9122, 29.7337, 0.0]\n",
      "========================================\n",
      "iter=10\n",
      "Previous decesion Rule=[0.0, 2.0, 1.0, 1.0, 0.0]\n",
      "New decesion Rule=[0, 1, 1, 1, 0]\n",
      "imme_reward=[27.1907, 2.0, 3.0, 4.0, 0.0]\n",
      "Optimal Value=[27.1907, 27.3259, 29.0891, 29.9753, 0.0]\n",
      "========================================\n",
      "iter=11\n",
      "Previous decesion Rule=[0.0, 1.0, 1.0, 1.0, 0.0]\n",
      "New decesion Rule=[0, 1, 1, 1, 0]\n",
      "imme_reward=[27.1907, 2.0, 3.0, 4.0, 0.0]\n",
      "Optimal Value=[27.1907, 27.3759, 29.0891, 29.9753, 0.0]\n",
      "========================================\n",
      "iter=12\n",
      "Previous decesion Rule=[0.0, 2.0, 1.0, 1.0, 0.0]\n",
      "New decesion Rule=[0, 1, 1, 1, 0]\n",
      "imme_reward=[27.3759, 2.0, 3.0, 4.0, 0.0]\n",
      "Optimal Value=[27.3759, 27.457, 29.1952, 30.1203, 0.0]\n",
      "========================================\n",
      "iter=13\n",
      "Previous decesion Rule=[0.0, 1.0, 1.0, 1.0, 0.0]\n",
      "New decesion Rule=[0, 1, 1, 1, 0]\n",
      "imme_reward=[27.3759, 2.0, 3.0, 4.0, 0.0]\n",
      "Optimal Value=[27.3759, 27.487, 29.1952, 30.1203, 0.0]\n",
      "========================================\n",
      "iter=14\n",
      "Previous decesion Rule=[0.0, 2.0, 1.0, 1.0, 0.0]\n",
      "New decesion Rule=[0, 1, 1, 1, 0]\n",
      "imme_reward=[27.487, 2.0, 3.0, 4.0, 0.0]\n",
      "Optimal Value=[27.487, 27.5356, 29.2589, 30.2072, 0.0]\n",
      "========================================\n",
      "iter=15\n",
      "Previous decesion Rule=[0.0, 1.0, 1.0, 1.0, 0.0]\n",
      "New decesion Rule=[0, 1, 1, 1, 0]\n",
      "imme_reward=[27.487, 2.0, 3.0, 4.0, 0.0]\n",
      "Optimal Value=[27.487, 27.5536, 29.2589, 30.2072, 0.0]\n",
      "========================================\n",
      "iter=16\n",
      "Previous decesion Rule=[0.0, 2.0, 1.0, 1.0, 0.0]\n",
      "New decesion Rule=[0, 1, 1, 1, 0]\n",
      "imme_reward=[27.5536, 2.0, 3.0, 4.0, 0.0]\n",
      "Optimal Value=[27.5536, 27.5828, 29.297, 30.2594, 0.0]\n",
      "========================================\n",
      "iter=17\n",
      "Previous decesion Rule=[0.0, 1.0, 1.0, 1.0, 0.0]\n",
      "New decesion Rule=[0, 1, 1, 1, 0]\n",
      "imme_reward=[27.5536, 2.0, 3.0, 4.0, 0.0]\n",
      "Optimal Value=[27.5536, 27.5936, 29.297, 30.2594, 0.0]\n",
      "========================================\n",
      "iter=18\n",
      "Previous decesion Rule=[0.0, 2.0, 1.0, 1.0, 0.0]\n",
      "New decesion Rule=[0, 1, 1, 1, 0]\n",
      "imme_reward=[27.5936, 2.0, 3.0, 4.0, 0.0]\n",
      "Optimal Value=[27.5936, 27.6111, 29.3199, 30.2906, 0.0]\n",
      "========================================\n",
      "iter=19\n",
      "Previous decesion Rule=[0.0, 1.0, 1.0, 1.0, 0.0]\n",
      "New decesion Rule=[0, 1, 1, 1, 0]\n",
      "imme_reward=[27.5936, 2.0, 3.0, 4.0, 0.0]\n",
      "Optimal Value=[27.5936, 27.6176, 29.3199, 30.2906, 0.0]\n",
      "========================================\n",
      "iter=20\n",
      "Previous decesion Rule=[0.0, 2.0, 1.0, 1.0, 0.0]\n",
      "New decesion Rule=[0, 1, 1, 1, 0]\n",
      "imme_reward=[27.6176, 2.0, 3.0, 4.0, 0.0]\n",
      "Optimal Value=[27.6176, 27.6281, 29.3337, 30.3094, 0.0]\n",
      "========================================\n",
      "iter=21\n",
      "Previous decesion Rule=[0.0, 1.0, 1.0, 1.0, 0.0]\n",
      "New decesion Rule=[0, 1, 1, 1, 0]\n",
      "imme_reward=[27.6176, 2.0, 3.0, 4.0, 0.0]\n",
      "Optimal Value=[27.6176, 27.6319, 29.3337, 30.3094, 0.0]\n",
      "========================================\n",
      "iter=22\n",
      "Previous decesion Rule=[0.0, 2.0, 1.0, 1.0, 0.0]\n",
      "New decesion Rule=[0, 1, 1, 1, 0]\n",
      "imme_reward=[27.6319, 2.0, 3.0, 4.0, 0.0]\n",
      "Optimal Value=[27.6319, 27.6382, 29.3419, 30.3207, 0.0]\n",
      "========================================\n",
      "iter=23\n",
      "Previous decesion Rule=[0.0, 1.0, 1.0, 1.0, 0.0]\n",
      "New decesion Rule=[0, 1, 1, 1, 0]\n",
      "imme_reward=[27.6319, 2.0, 3.0, 4.0, 0.0]\n",
      "Optimal Value=[27.6319, 27.6406, 29.3419, 30.3207, 0.0]\n",
      "========================================\n",
      "iter=24\n",
      "Previous decesion Rule=[0.0, 2.0, 1.0, 1.0, 0.0]\n",
      "New decesion Rule=[0, 1, 1, 1, 0]\n",
      "imme_reward=[27.6406, 2.0, 3.0, 4.0, 0.0]\n",
      "Optimal Value=[27.6406, 27.6443, 29.3468, 30.3274, 0.0]\n",
      "========================================\n",
      "iter=25\n",
      "Previous decesion Rule=[0.0, 1.0, 1.0, 1.0, 0.0]\n",
      "New decesion Rule=[0, 1, 1, 1, 0]\n",
      "imme_reward=[27.6406, 2.0, 3.0, 4.0, 0.0]\n",
      "Optimal Value=[27.6406, 27.6457, 29.3468, 30.3274, 0.0]\n",
      "========================================\n",
      "iter=26\n",
      "Previous decesion Rule=[0.0, 2.0, 1.0, 1.0, 0.0]\n",
      "New decesion Rule=[0, 1, 1, 1, 0]\n",
      "imme_reward=[27.6457, 2.0, 3.0, 4.0, 0.0]\n",
      "Optimal Value=[27.6457, 27.648, 29.3498, 30.3315, 0.0]\n",
      "========================================\n",
      "iter=27\n",
      "Previous decesion Rule=[0.0, 1.0, 1.0, 1.0, 0.0]\n",
      "New decesion Rule=[0, 1, 1, 1, 0]\n",
      "imme_reward=[27.6457, 2.0, 3.0, 4.0, 0.0]\n",
      "Optimal Value=[27.6457, 27.6488, 29.3498, 30.3315, 0.0]\n",
      "========================================\n",
      "iter=28\n",
      "Previous decesion Rule=[0.0, 2.0, 1.0, 1.0, 0.0]\n",
      "New decesion Rule=[0, 1, 1, 1, 0]\n",
      "imme_reward=[27.6488, 2.0, 3.0, 4.0, 0.0]\n",
      "Optimal Value=[27.6488, 27.6502, 29.3516, 30.3339, 0.0]\n",
      "========================================\n",
      "iter=29\n",
      "Previous decesion Rule=[0.0, 1.0, 1.0, 1.0, 0.0]\n",
      "New decesion Rule=[0, 1, 1, 1, 0]\n",
      "imme_reward=[27.6488, 2.0, 3.0, 4.0, 0.0]\n",
      "Optimal Value=[27.6488, 27.6507, 29.3516, 30.3339, 0.0]\n",
      "========================================\n",
      "iter=30\n",
      "Previous decesion Rule=[0.0, 2.0, 1.0, 1.0, 0.0]\n",
      "New decesion Rule=[0, 1, 1, 1, 0]\n",
      "imme_reward=[27.6507, 2.0, 3.0, 4.0, 0.0]\n",
      "Optimal Value=[27.6507, 27.6515, 29.3527, 30.3353, 0.0]\n",
      "========================================\n",
      "iter=31\n",
      "Previous decesion Rule=[0.0, 1.0, 1.0, 1.0, 0.0]\n",
      "New decesion Rule=[0, 1, 1, 1, 0]\n",
      "imme_reward=[27.6507, 2.0, 3.0, 4.0, 0.0]\n",
      "Optimal Value=[27.6507, 27.6518, 29.3527, 30.3353, 0.0]\n",
      "========================================\n",
      "iter=32\n",
      "Previous decesion Rule=[0.0, 2.0, 1.0, 1.0, 0.0]\n",
      "New decesion Rule=[0, 1, 1, 1, 0]\n",
      "imme_reward=[27.6518, 2.0, 3.0, 4.0, 0.0]\n",
      "Optimal Value=[27.6518, 27.6523, 29.3533, 30.3362, 0.0]\n",
      "========================================\n",
      "iter=33\n",
      "Previous decesion Rule=[0.0, 1.0, 1.0, 1.0, 0.0]\n",
      "New decesion Rule=[0, 1, 1, 1, 0]\n",
      "imme_reward=[27.6518, 2.0, 3.0, 4.0, 0.0]\n",
      "Optimal Value=[27.6518, 27.6525, 29.3533, 30.3362, 0.0]\n",
      "========================================\n",
      "iter=34\n",
      "Previous decesion Rule=[0.0, 2.0, 1.0, 1.0, 0.0]\n",
      "New decesion Rule=[0, 1, 1, 1, 0]\n",
      "imme_reward=[27.6525, 2.0, 3.0, 4.0, 0.0]\n",
      "Optimal Value=[27.6525, 27.6528, 29.3537, 30.3367, 0.0]\n",
      "========================================\n",
      "iter=35\n",
      "Previous decesion Rule=[0.0, 1.0, 1.0, 1.0, 0.0]\n",
      "New decesion Rule=[0, 1, 1, 1, 0]\n",
      "imme_reward=[27.6525, 2.0, 3.0, 4.0, 0.0]\n",
      "Optimal Value=[27.6525, 27.6529, 29.3537, 30.3367, 0.0]\n",
      "========================================\n",
      "iter=36\n",
      "Previous decesion Rule=[0.0, 2.0, 1.0, 1.0, 0.0]\n",
      "New decesion Rule=[0, 1, 1, 1, 0]\n",
      "imme_reward=[27.6529, 2.0, 3.0, 4.0, 0.0]\n",
      "Optimal Value=[27.6529, 27.6531, 29.3539, 30.337, 0.0]\n",
      "========================================\n",
      "iter=37\n",
      "Previous decesion Rule=[0.0, 1.0, 1.0, 1.0, 0.0]\n",
      "New decesion Rule=[0, 1, 1, 1, 0]\n",
      "imme_reward=[27.6529, 2.0, 3.0, 4.0, 0.0]\n",
      "Optimal Value=[27.6529, 27.6531, 29.3539, 30.337, 0.0]\n",
      "========================================\n",
      "iter=38\n",
      "Previous decesion Rule=[0.0, 2.0, 1.0, 1.0, 0.0]\n",
      "New decesion Rule=[0, 1, 1, 1, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imme_reward=[27.6531, 2.0, 3.0, 4.0, 0.0]\n",
      "Optimal Value=[27.6531, 27.6532, 29.354, 30.3372, 0.0]\n",
      "========================================\n",
      "iter=39\n",
      "Previous decesion Rule=[0.0, 1.0, 1.0, 1.0, 0.0]\n",
      "New decesion Rule=[0, 1, 1, 1, 0]\n",
      "imme_reward=[27.6531, 2.0, 3.0, 4.0, 0.0]\n",
      "Optimal Value=[27.6531, 27.6533, 29.354, 30.3372, 0.0]\n",
      "========================================\n",
      "iter=40\n",
      "Previous decesion Rule=[0.0, 2.0, 1.0, 1.0, 0.0]\n",
      "New decesion Rule=[0, 1, 1, 1, 0]\n",
      "imme_reward=[27.6533, 2.0, 3.0, 4.0, 0.0]\n",
      "Optimal Value=[27.6533, 27.6533, 29.3541, 30.3373, 0.0]\n",
      "========================================\n",
      "iter=41\n",
      "Previous decesion Rule=[0.0, 1.0, 1.0, 1.0, 0.0]\n",
      "New decesion Rule=[0, 1, 1, 1, 0]\n",
      "imme_reward=[27.6533, 2.0, 3.0, 4.0, 0.0]\n",
      "Optimal Value=[27.6533, 27.6534, 29.3541, 30.3373, 0.0]\n",
      "========================================\n",
      "iter=42\n",
      "Previous decesion Rule=[0.0, 2.0, 1.0, 1.0, 0.0]\n",
      "New decesion Rule=[0, 1, 1, 1, 0]\n",
      "imme_reward=[27.6534, 2.0, 3.0, 4.0, 0.0]\n",
      "Optimal Value=[27.6534, 27.6534, 29.3542, 30.3374, 0.0]\n",
      "========================================\n",
      "iter=43\n",
      "Previous decesion Rule=[0.0, 1.0, 1.0, 1.0, 0.0]\n",
      "New decesion Rule=[0, 1, 1, 1, 0]\n",
      "imme_reward=[27.6534, 2.0, 3.0, 4.0, 0.0]\n",
      "Optimal Value=[27.6534, 27.6534, 29.3542, 30.3374, 0.0]\n",
      "========================================\n",
      "iter=44\n",
      "Previous decesion Rule=[0.0, 2.0, 1.0, 1.0, 0.0]\n",
      "New decesion Rule=[0, 1, 1, 1, 0]\n",
      "imme_reward=[27.6534, 2.0, 3.0, 4.0, 0.0]\n",
      "Optimal Value=[27.6534, 27.6534, 29.3542, 30.3375, 0.0]\n",
      "========================================\n",
      "iter=45\n",
      "Previous decesion Rule=[0.0, 1.0, 1.0, 1.0, 0.0]\n",
      "New decesion Rule=[0, 1, 1, 1, 0]\n",
      "imme_reward=[27.6534, 2.0, 3.0, 4.0, 0.0]\n",
      "Optimal Value=[27.6534, 27.6534, 29.3542, 30.3375, 0.0]\n",
      "========================================\n",
      "iter=46\n",
      "Previous decesion Rule=[0.0, 2.0, 1.0, 1.0, 0.0]\n",
      "New decesion Rule=[0, 1, 1, 1, 0]\n",
      "imme_reward=[27.6534, 2.0, 3.0, 4.0, 0.0]\n",
      "Optimal Value=[27.6534, 27.6535, 29.3542, 30.3375, 0.0]\n",
      "========================================\n",
      "iter=47\n",
      "Previous decesion Rule=[0.0, 1.0, 1.0, 1.0, 0.0]\n",
      "New decesion Rule=[0, 1, 1, 1, 0]\n",
      "imme_reward=[27.6534, 2.0, 3.0, 4.0, 0.0]\n",
      "Optimal Value=[27.6534, 27.6535, 29.3542, 30.3375, 0.0]\n",
      "========================================\n",
      "iter=48\n",
      "Previous decesion Rule=[0.0, 2.0, 1.0, 1.0, 0.0]\n",
      "New decesion Rule=[0, 1, 1, 1, 0]\n",
      "imme_reward=[27.6535, 2.0, 3.0, 4.0, 0.0]\n",
      "Optimal Value=[27.6535, 27.6535, 29.3542, 30.3375, 0.0]\n",
      "========================================\n",
      "iter=49\n",
      "Previous decesion Rule=[0.0, 1.0, 1.0, 1.0, 0.0]\n",
      "New decesion Rule=[0, 1, 1, 1, 0]\n",
      "imme_reward=[27.6535, 2.0, 3.0, 4.0, 0.0]\n",
      "Optimal Value=[27.6535, 27.6535, 29.3542, 30.3375, 0.0]\n",
      "========================================\n",
      "iter=50\n",
      "Previous decesion Rule=[0.0, 2.0, 1.0, 1.0, 0.0]\n",
      "New decesion Rule=[0, 0, 1, 1, 0]\n",
      "imme_reward=[27.6535, 27.6535, 3.0, 4.0, 0.0]\n",
      "Optimal Value=[27.6535, 27.6535, 29.3542, 30.3375, 0.0]\n",
      "========================================\n",
      "iter=51\n",
      "Previous decesion Rule=[0.0, 0.0, 1.0, 1.0, 0.0]\n",
      "New decesion Rule=[0, 0, 1, 1, 0]\n",
      "imme_reward=[27.6535, 27.6535, 3.0, 4.0, 0.0]\n",
      "Optimal Value=[27.6535, 27.6535, 29.3542, 30.3375, 0.0]\n",
      "========================================\n",
      "Number of iterations=51\n",
      "Optimal policy=[0, 0, 1, 1, 0]\n",
      "Optimal Value=[27.6535, 27.6535, 29.3542, 30.3375, 0.0]\n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "while true \n",
    "    iter += 1\n",
    "    println(\"iter=\",iter)\n",
    "    dec_copy = zeros(nbstates);\n",
    "    for s=1:nbstates\n",
    "        dec_copy[s]=dec[s]\n",
    "    end\n",
    "    \n",
    "    #step1 policy evaluation  \n",
    "    V = inv(Id-lambda*PP)*imme_reward;\n",
    "    for s=1:nbstates-1\n",
    "        action = zeros(nbaction);\n",
    "        for a=1:nbaction\n",
    "            if a==1\n",
    "                imme_reward[s] = reward[s]\n",
    "                for ss=1:nbstates-1\n",
    "                    PP[s,ss]=P[s,ss]\n",
    "                    PP[s,5] = 0\n",
    "                end\n",
    "            else\n",
    "                imme_reward[s] = R\n",
    "                for ss=1:nbstates-1\n",
    "                    PP[s,ss]=0\n",
    "                end\n",
    "                PP[s,5]=1\n",
    "            end\n",
    "            action[a] = imme_reward[s]+lambda*PP[s,:]'*V\n",
    "        end\n",
    "        dec[s] = findmax(action)[2]\n",
    "        V[s] = findmax(action)[1]\n",
    "        \n",
    "        if dec[s]==1\n",
    "            imme_reward[s] = reward[s]\n",
    "        else\n",
    "            imme_reward[s] = R\n",
    "        end \n",
    "    end\n",
    "    #println(\"iter=\",iter)\n",
    "    for i=1:length(dec)\n",
    "        if dec[i]==2\n",
    "            dec[i]=0\n",
    "        end\n",
    "    end\n",
    "    println(\"Previous decesion Rule=\",dec_copy)\n",
    "    println(\"New decesion Rule=\",dec)\n",
    "    println(\"imme_reward=\",imme_reward)\n",
    "    println(\"Optimal Value=\", V)\n",
    "    println(\"========================================\")\n",
    "    if dec == dec_copy && dec[2]!=0\n",
    "        \n",
    "        action = zeros(nbaction);\n",
    "        for a=1:nbaction\n",
    "            if a==1\n",
    "                imme_reward[2] = reward[2]\n",
    "                for ss=1:nbstates-1\n",
    "                    PP[2,ss]=P[2,ss]\n",
    "                    PP[2,5] = 0\n",
    "                end\n",
    "            else\n",
    "                imme_reward[2] = R\n",
    "                for ss=1:nbstates-1\n",
    "                    PP[2,ss]=0\n",
    "                end\n",
    "                PP[2,5]=1\n",
    "            end\n",
    "            action[a] = imme_reward[2]+lambda*PP[2,:]'*V\n",
    "        end\n",
    "        dec[2] = 2\n",
    "        R = findmax(action)[1]+1e-5\n",
    "        \n",
    "    elseif dec == dec_copy && dec[2]==0\n",
    "        break;\n",
    "    end\n",
    "    #transition probability \n",
    "\n",
    "    for s=1:nbstates-1\n",
    "        if dec[s]==1\n",
    "            imme_reward[s] = reward[s]\n",
    "            for ss=1:nbstates-1\n",
    "                PP[s,ss]=P[s,ss]\n",
    "                PP[s,5] = 0\n",
    "            end\n",
    "        else\n",
    "            imme_reward[s] = R\n",
    "            for ss=1:nbstates-1\n",
    "                PP[s,ss]=0\n",
    "                PP[s,5]=1\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end \n",
    "\n",
    "println(\"Number of iterations=\", iter)\n",
    "println(\"Optimal policy=\", dec)\n",
    "println(\"Optimal Value=\", V)\n",
    "println(\"=====================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.5",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
